# -*- coding: utf-8 -*-
"""week07.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19lLTJbrQGFcjOVESdnXi1yq-Uxf8ByZi
"""

# Use pytorch to realize AlexNet

# 查看当前GPU信息
!nvidia-smi

import torch
from torch import nn, optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt

# 定义AlexNet阉割版模型
class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()
        # 5个卷积层
        self.output_dim1 = [1, 64, 128, 256]
        self.conv = nn.Sequential(
            nn.Conv2d(self.output_dim1[0], self.output_dim1[1], kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(self.output_dim1[1], self.output_dim1[2], kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(self.output_dim1[2], self.output_dim1[3], kernel_size=3, stride=1, padding=1),
            nn.ReLU(),

            # nn.Conv2d(output_dim1[3], output_dim1[4], kernel_size=3, stride=1, padding=1),
            # nn.ReLU(),

            # nn.Conv2d(output_dim1[4], output_dim1[5], kernel_size=3, stride=1, padding=1),
            # nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        # 3个全连接层
        self.output_dim2 = [256*3*3, 256, 10]
        self.fc = nn.Sequential(
            nn.Linear(self.output_dim2[0], self.output_dim2[1]),
            nn.Linear(self.output_dim2[1], self.output_dim2[2]),
            # nn.Linear(output_dim2[2], output_dim2[3]),
        )
    
    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        y = self.fc(x)
        return y

    # 显示每一层参数数量
    def showLayers(self):
        n_conv, n_fc = len(self.output_dim1), len(self.output_dim2)
        print(f"There are {n_conv-1} Conv layers and {n_fc-1} FC layers.")
        print("Parameter number for each layer is:")
        i = 1
        for x, y in zip(self.output_dim1, self.output_dim1[1:]):
            print(f"Layer Conv{i} has {3*3*x*y + y} parameters...")
            i += 1
        i = 1
        for x, y in zip(self.output_dim2, self.output_dim2[1:]):
            print(f"Layer FC{i} has {x*y + y} parameters...")
            i += 1

# 定义训练函数
class Trainer:
    def __init__(self, model, criterion):
        self.model = model
        self.criterion = criterion

    def train(self, epoches, data_loader, optimizer):
        loss_curve = []
        acc_curve = []

        for i in range(epoches):
            SUM = runloss = correct = 0
            for input, label in data_loader:
                # 将Tensor类型数据转为可计算梯度、进行梯度下降的数据类型
                input = torch.autograd.Variable(input)
                label = torch.autograd.Variable(label)
                # 前向计算
                output = model(input)
                # 计算损失
                loss = self.criterion(output, label)
                # 梯度先清零再进行反向传播
                optimizer.zero_grad()
                loss.backward()
                # 梯度下降（学习一步）
                optimizer.step()

                # 画曲线、计算训练效果用到的参数
                runloss += loss.data.item()
                _, predict = torch.max(output, 1)
                correct_num = (predict == label).sum()
                correct += correct_num.data.item()
                SUM += len(label)
            
            epoch_loss = runloss / SUM
            loss_curve.append(epoch_loss)
            epoch_correct = correct / SUM
            acc_curve.append(epoch_correct)
            print(f"epoch: {i}, loss: {epoch_loss}, acc: {epoch_correct}")
        return loss_curve, acc_curve

if __name__ == '__main__':
    # 定义全局参数
    lr = 1e-2
    batch_size = 64
    epoches = 2

    # 加载mnist数据集
    # 定义数据初始化变化，1.ToTensor 2.normalize
    transfer = transforms.Compose([transforms.ToTensor(), 
                        transforms.Normalize(mean=(0.5,), std=(0.5,))])

    train_data = datasets.MNIST(
        root='./data/mnist',
        train=True,
        transform=transfer,
        download=True
    )
    test_data = datasets.MNIST(
        root='./data/mnist',
        train=False,
        transform=transfer,
        download=True
    )

    train_loader = torch.utils.data.DataLoader(
        dataset=train_data,
        batch_size=batch_size,
        shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        dataset=test_data,
        batch_size=batch_size,
        shuffle=False
    )

    # 实例化模型
    model = AlexNet()

    # 定义损失函数(交叉熵损失)
    criterion = nn.CrossEntropyLoss()

    # 定义优化器
    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)

    trainer = Trainer(model, criterion)

    print("Starting train...")
    loss_curve, acc_curve = trainer.train(epoches, train_loader, optimizer)
    print("End train.")

    # 打印model中每一层的参数量
    model.showLayers()

    # 显示模型各层参数、计算量等信息
    from torchstat import stat
    stat(model, (1, 28, 28))